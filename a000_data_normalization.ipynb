{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708f84c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting dataset normalization process...\n",
      "Normalization formula: col_norm = col / max(abs(col_all_datasets))\n",
      "Expected range: [-1, 1]\n",
      "\n",
      "Found 69 CSV files:\n",
      "  - dataset_10_s1.csv\n",
      "  - dataset_11_s1.csv\n",
      "  - dataset_12_s1.csv\n",
      "  - dataset_13_s1.csv\n",
      "  - dataset_14_s1.csv\n",
      "  - dataset_15_s1.csv\n",
      "  - dataset_16_s1.csv\n",
      "  - dataset_17_s1.csv\n",
      "  - dataset_18_s1.csv\n",
      "  - dataset_19_s1.csv\n",
      "  - dataset_20_s1.csv\n",
      "  - dataset_21_s1.csv\n",
      "  - dataset_23_s1.csv\n",
      "  - dataset_24_s1.csv\n",
      "  - dataset_26_s1.csv\n",
      "  - dataset_27_s1.csv\n",
      "  - dataset_29_s1.csv\n",
      "  - dataset_2_s1.csv\n",
      "  - dataset_30_s1.csv\n",
      "  - dataset_31_s1.csv\n",
      "  - dataset_32_s1.csv\n",
      "  - dataset_36_s1.csv\n",
      "  - dataset_3_s1.csv\n",
      "  - dataset_41_s1.csv\n",
      "  - dataset_42_s1.csv\n",
      "  - dataset_43_s1.csv\n",
      "  - dataset_44_s1.csv\n",
      "  - dataset_45_s1.csv\n",
      "  - dataset_46_s1.csv\n",
      "  - dataset_47_s1.csv\n",
      "  - dataset_48_s1.csv\n",
      "  - dataset_49_s1.csv\n",
      "  - dataset_4_s1.csv\n",
      "  - dataset_50_s1.csv\n",
      "  - dataset_51_s1.csv\n",
      "  - dataset_52_s1.csv\n",
      "  - dataset_53_s1.csv\n",
      "  - dataset_54_s1.csv\n",
      "  - dataset_55_s1.csv\n",
      "  - dataset_56_s1.csv\n",
      "  - dataset_57_s1.csv\n",
      "  - dataset_58_s1.csv\n",
      "  - dataset_59_s1.csv\n",
      "  - dataset_5_s1.csv\n",
      "  - dataset_60_s1.csv\n",
      "  - dataset_61_s1.csv\n",
      "  - dataset_62_s1.csv\n",
      "  - dataset_63_s1.csv\n",
      "  - dataset_64_s1.csv\n",
      "  - dataset_65_s1.csv\n",
      "  - dataset_66_s1.csv\n",
      "  - dataset_67_s1.csv\n",
      "  - dataset_68_s1.csv\n",
      "  - dataset_69_s1.csv\n",
      "  - dataset_6_s1.csv\n",
      "  - dataset_70_s1.csv\n",
      "  - dataset_71_s1.csv\n",
      "  - dataset_72_s1.csv\n",
      "  - dataset_73_s1.csv\n",
      "  - dataset_74_s1.csv\n",
      "  - dataset_75_s1.csv\n",
      "  - dataset_76_s1.csv\n",
      "  - dataset_78_s1.csv\n",
      "  - dataset_79_s1.csv\n",
      "  - dataset_7_s1.csv\n",
      "  - dataset_80_s1.csv\n",
      "  - dataset_81_s1.csv\n",
      "  - dataset_8_s1.csv\n",
      "  - dataset_9_s1.csv\n",
      "\n",
      "Step 1: Loading all datasets to find global absolute maxima...\n",
      "  Loaded dataset_10_s1.csv: 15256 rows, 12 columns\n",
      "  Loaded dataset_11_s1.csv: 7887 rows, 12 columns\n",
      "  Loaded dataset_12_s1.csv: 21942 rows, 12 columns\n",
      "  Loaded dataset_13_s1.csv: 35906 rows, 12 columns\n",
      "  Loaded dataset_14_s1.csv: 18598 rows, 12 columns\n",
      "  Loaded dataset_15_s1.csv: 18124 rows, 12 columns\n",
      "  Loaded dataset_16_s1.csv: 20645 rows, 12 columns\n",
      "  Loaded dataset_17_s1.csv: 15964 rows, 12 columns\n",
      "  Loaded dataset_18_s1.csv: 37732 rows, 12 columns\n",
      "  Loaded dataset_19_s1.csv: 10410 rows, 12 columns\n",
      "  Loaded dataset_20_s1.csv: 43971 rows, 12 columns\n",
      "  Loaded dataset_21_s1.csv: 17321 rows, 12 columns\n",
      "  Loaded dataset_23_s1.csv: 11856 rows, 12 columns\n",
      "  Loaded dataset_24_s1.csv: 15015 rows, 12 columns\n",
      "  Loaded dataset_26_s1.csv: 16666 rows, 12 columns\n",
      "  Loaded dataset_27_s1.csv: 35361 rows, 12 columns\n",
      "  Loaded dataset_29_s1.csv: 21358 rows, 12 columns\n",
      "  Loaded dataset_2_s1.csv: 19357 rows, 12 columns\n",
      "  Loaded dataset_30_s1.csv: 23863 rows, 12 columns\n",
      "  Loaded dataset_31_s1.csv: 15587 rows, 12 columns\n",
      "  Loaded dataset_32_s1.csv: 20960 rows, 12 columns\n",
      "  Loaded dataset_36_s1.csv: 22609 rows, 12 columns\n",
      "  Loaded dataset_3_s1.csv: 19248 rows, 12 columns\n",
      "  Loaded dataset_41_s1.csv: 16700 rows, 12 columns\n",
      "  Loaded dataset_42_s1.csv: 16920 rows, 12 columns\n",
      "  Loaded dataset_43_s1.csv: 8443 rows, 12 columns\n",
      "  Loaded dataset_44_s1.csv: 26341 rows, 12 columns\n",
      "  Loaded dataset_45_s1.csv: 17142 rows, 12 columns\n",
      "  Loaded dataset_46_s1.csv: 2180 rows, 12 columns\n",
      "  Loaded dataset_47_s1.csv: 2176 rows, 12 columns\n",
      "  Loaded dataset_48_s1.csv: 21983 rows, 12 columns\n",
      "  Loaded dataset_49_s1.csv: 10816 rows, 12 columns\n",
      "  Loaded dataset_4_s1.csv: 33424 rows, 12 columns\n",
      "  Loaded dataset_50_s1.csv: 10810 rows, 12 columns\n",
      "  Loaded dataset_51_s1.csv: 6261 rows, 12 columns\n",
      "  Loaded dataset_52_s1.csv: 3726 rows, 12 columns\n",
      "  Loaded dataset_53_s1.csv: 32442 rows, 12 columns\n",
      "  Loaded dataset_54_s1.csv: 10807 rows, 12 columns\n",
      "  Loaded dataset_55_s1.csv: 10807 rows, 12 columns\n",
      "  Loaded dataset_56_s1.csv: 33123 rows, 12 columns\n",
      "  Loaded dataset_57_s1.csv: 14403 rows, 12 columns\n",
      "  Loaded dataset_58_s1.csv: 33382 rows, 12 columns\n",
      "  Loaded dataset_59_s1.csv: 7475 rows, 12 columns\n",
      "  Loaded dataset_5_s1.csv: 14788 rows, 12 columns\n",
      "  Loaded dataset_60_s1.csv: 14543 rows, 12 columns\n",
      "  Loaded dataset_61_s1.csv: 14516 rows, 12 columns\n",
      "  Loaded dataset_62_s1.csv: 25600 rows, 12 columns\n",
      "  Loaded dataset_63_s1.csv: 16668 rows, 12 columns\n",
      "  Loaded dataset_64_s1.csv: 6250 rows, 12 columns\n",
      "  Loaded dataset_65_s1.csv: 40094 rows, 12 columns\n",
      "  Loaded dataset_66_s1.csv: 36476 rows, 12 columns\n",
      "  Loaded dataset_67_s1.csv: 11135 rows, 12 columns\n",
      "  Loaded dataset_68_s1.csv: 23331 rows, 12 columns\n",
      "  Loaded dataset_69_s1.csv: 15350 rows, 12 columns\n",
      "  Loaded dataset_6_s1.csv: 40388 rows, 12 columns\n",
      "  Loaded dataset_70_s1.csv: 25677 rows, 12 columns\n",
      "  Loaded dataset_71_s1.csv: 14656 rows, 12 columns\n",
      "  Loaded dataset_72_s1.csv: 15301 rows, 12 columns\n",
      "  Loaded dataset_73_s1.csv: 16786 rows, 12 columns\n",
      "  Loaded dataset_74_s1.csv: 23761 rows, 12 columns\n",
      "  Loaded dataset_75_s1.csv: 13472 rows, 12 columns\n",
      "  Loaded dataset_76_s1.csv: 22188 rows, 12 columns\n",
      "  Loaded dataset_78_s1.csv: 8445 rows, 12 columns\n",
      "  Loaded dataset_79_s1.csv: 31154 rows, 12 columns\n",
      "  Loaded dataset_7_s1.csv: 14651 rows, 12 columns\n",
      "  Loaded dataset_80_s1.csv: 23824 rows, 12 columns\n",
      "  Loaded dataset_81_s1.csv: 17672 rows, 12 columns\n",
      "  Loaded dataset_8_s1.csv: 18757 rows, 12 columns\n",
      "  Loaded dataset_9_s1.csv: 20336 rows, 12 columns\n",
      "\n",
      "Combining 69 datasets (1330816 total rows)...\n",
      "\n",
      "Global absolute maxima across all datasets:\n",
      "============================================================\n",
      "u_q            : max(abs) =   133.036994 | range: [ -25.291,  133.037]\n",
      "coolant        : max(abs) =   101.598512 | range: [  10.624,  101.599]\n",
      "stator_winding : max(abs) =   141.362885 | range: [  18.586,  141.363]\n",
      "u_d            : max(abs) =   131.530411 | range: [-131.530,  131.470]\n",
      "stator_tooth   : max(abs) =   111.946423 | range: [  18.134,  111.946]\n",
      "motor_speed    : max(abs) =  6000.015137 | range: [-275.549, 6000.015]\n",
      "i_d            : max(abs) =   278.003632 | range: [-278.004,    0.052]\n",
      "i_q            : max(abs) =   301.707855 | range: [-293.427,  301.708]\n",
      "pm             : max(abs) =   113.606628 | range: [  20.857,  113.607]\n",
      "stator_yoke    : max(abs) =   101.147964 | range: [  18.077,  101.148]\n",
      "ambient        : max(abs) =    30.714205 | range: [   8.783,   30.714]\n",
      "torque         : max(abs) =   261.005707 | range: [-246.467,  261.006]\n",
      "============================================================\n",
      "\n",
      "Created/verified output folder: EM_data_norm/\n",
      "\n",
      "Step 2: Normalizing and saving datasets...\n",
      "  dataset_10_s1.csv         -> dataset_10_s1_norm.csv         | Shape: (15256, 12) | Range: [-0.9968,  0.9995]\n",
      "  dataset_11_s1.csv         -> dataset_11_s1_norm.csv         | Shape: (7887, 12) | Range: [-0.9888,  0.9897]\n",
      "  dataset_12_s1.csv         -> dataset_12_s1_norm.csv         | Shape: (21942, 12) | Range: [-0.6494,  0.7863]\n",
      "  dataset_13_s1.csv         -> dataset_13_s1_norm.csv         | Shape: (35906, 12) | Range: [-0.1565,  0.7631]\n",
      "  dataset_14_s1.csv         -> dataset_14_s1_norm.csv         | Shape: (18598, 12) | Range: [-0.8520,  0.9927]\n",
      "  dataset_15_s1.csv         -> dataset_15_s1_norm.csv         | Shape: (18124, 12) | Range: [-0.9923,  0.9917]\n",
      "  dataset_16_s1.csv         -> dataset_16_s1_norm.csv         | Shape: (20645, 12) | Range: [-0.3110,  0.8240]\n",
      "  dataset_17_s1.csv         -> dataset_17_s1_norm.csv         | Shape: (15964, 12) | Range: [-0.7381,  0.9872]\n",
      "  dataset_18_s1.csv         -> dataset_18_s1_norm.csv         | Shape: (37732, 12) | Range: [-0.2876,  0.7769]\n",
      "  dataset_19_s1.csv         -> dataset_19_s1_norm.csv         | Shape: (10410, 12) | Range: [-0.8931,  1.0000]\n",
      "  dataset_20_s1.csv         -> dataset_20_s1_norm.csv         | Shape: (43971, 12) | Range: [-0.9890,  1.0000]\n",
      "  dataset_21_s1.csv         -> dataset_21_s1_norm.csv         | Shape: (17321, 12) | Range: [-0.9744,  0.9894]\n",
      "  dataset_23_s1.csv         -> dataset_23_s1_norm.csv         | Shape: (11856, 12) | Range: [-0.9978,  0.9913]\n",
      "  dataset_24_s1.csv         -> dataset_24_s1_norm.csv         | Shape: (15015, 12) | Range: [-0.9939,  1.0000]\n",
      "  dataset_26_s1.csv         -> dataset_26_s1_norm.csv         | Shape: (16666, 12) | Range: [-0.9973,  0.9913]\n",
      "  dataset_27_s1.csv         -> dataset_27_s1_norm.csv         | Shape: (35361, 12) | Range: [-0.9982,  0.9808]\n",
      "  dataset_29_s1.csv         -> dataset_29_s1_norm.csv         | Shape: (21358, 12) | Range: [-0.9969,  1.0000]\n",
      "  dataset_2_s1.csv          -> dataset_2_s1_norm.csv          | Shape: (19357, 12) | Range: [-0.4423,  0.8035]\n",
      "  dataset_30_s1.csv         -> dataset_30_s1_norm.csv         | Shape: (23863, 12) | Range: [-0.7899,  0.8406]\n",
      "  dataset_31_s1.csv         -> dataset_31_s1_norm.csv         | Shape: (15587, 12) | Range: [-0.9775,  1.0000]\n",
      "  dataset_32_s1.csv         -> dataset_32_s1_norm.csv         | Shape: (20960, 12) | Range: [-0.9979,  0.9343]\n",
      "  dataset_36_s1.csv         -> dataset_36_s1_norm.csv         | Shape: (22609, 12) | Range: [-0.9976,  1.0000]\n",
      "  dataset_3_s1.csv          -> dataset_3_s1_norm.csv          | Shape: (19248, 12) | Range: [-0.1565,  0.7457]\n",
      "  dataset_41_s1.csv         -> dataset_41_s1_norm.csv         | Shape: (16700, 12) | Range: [-0.9943,  0.9913]\n",
      "  dataset_42_s1.csv         -> dataset_42_s1_norm.csv         | Shape: (16920, 12) | Range: [-0.8894,  0.9363]\n",
      "  dataset_43_s1.csv         -> dataset_43_s1_norm.csv         | Shape: (8443, 12) | Range: [-0.9954,  0.9884]\n",
      "  dataset_44_s1.csv         -> dataset_44_s1_norm.csv         | Shape: (26341, 12) | Range: [-0.9814,  0.9896]\n",
      "  dataset_45_s1.csv         -> dataset_45_s1_norm.csv         | Shape: (17142, 12) | Range: [-0.9657,  0.9169]\n",
      "  dataset_46_s1.csv         -> dataset_46_s1_norm.csv         | Shape: (2180, 12) | Range: [-0.9893,  0.9912]\n",
      "  dataset_47_s1.csv         -> dataset_47_s1_norm.csv         | Shape: (2176, 12) | Range: [-0.9911,  0.9871]\n",
      "  dataset_48_s1.csv         -> dataset_48_s1_norm.csv         | Shape: (21983, 12) | Range: [-0.9910,  0.9960]\n",
      "  dataset_49_s1.csv         -> dataset_49_s1_norm.csv         | Shape: (10816, 12) | Range: [-0.9537,  0.9948]\n",
      "  dataset_4_s1.csv          -> dataset_4_s1_norm.csv          | Shape: (33424, 12) | Range: [-0.9590,  1.0000]\n",
      "  dataset_50_s1.csv         -> dataset_50_s1_norm.csv         | Shape: (10810, 12) | Range: [-0.9524,  0.9947]\n",
      "  dataset_51_s1.csv         -> dataset_51_s1_norm.csv         | Shape: (6261, 12) | Range: [-0.9957,  1.0000]\n",
      "  dataset_52_s1.csv         -> dataset_52_s1_norm.csv         | Shape: (3726, 12) | Range: [-0.9905,  0.9847]\n",
      "  dataset_53_s1.csv         -> dataset_53_s1_norm.csv         | Shape: (32442, 12) | Range: [-0.9967,  0.9902]\n",
      "  dataset_54_s1.csv         -> dataset_54_s1_norm.csv         | Shape: (10807, 12) | Range: [-0.9935,  0.9912]\n",
      "  dataset_55_s1.csv         -> dataset_55_s1_norm.csv         | Shape: (10807, 12) | Range: [-0.9940,  0.9926]\n",
      "  dataset_56_s1.csv         -> dataset_56_s1_norm.csv         | Shape: (33123, 12) | Range: [-0.9977,  0.9998]\n",
      "  dataset_57_s1.csv         -> dataset_57_s1_norm.csv         | Shape: (14403, 12) | Range: [-0.9909,  0.9903]\n",
      "  dataset_58_s1.csv         -> dataset_58_s1_norm.csv         | Shape: (33382, 12) | Range: [-0.9967,  1.0000]\n",
      "  dataset_59_s1.csv         -> dataset_59_s1_norm.csv         | Shape: (7475, 12) | Range: [-0.9947,  1.0000]\n",
      "  dataset_5_s1.csv          -> dataset_5_s1_norm.csv          | Shape: (14788, 12) | Range: [-0.3283,  0.7723]\n",
      "  dataset_60_s1.csv         -> dataset_60_s1_norm.csv         | Shape: (14543, 12) | Range: [-0.9946,  0.9987]\n",
      "  dataset_61_s1.csv         -> dataset_61_s1_norm.csv         | Shape: (14516, 12) | Range: [-0.9942,  0.9885]\n",
      "  dataset_62_s1.csv         -> dataset_62_s1_norm.csv         | Shape: (25600, 12) | Range: [-0.9882,  0.9955]\n",
      "  dataset_63_s1.csv         -> dataset_63_s1_norm.csv         | Shape: (16668, 12) | Range: [-0.9938,  0.9926]\n",
      "  dataset_64_s1.csv         -> dataset_64_s1_norm.csv         | Shape: (6250, 12) | Range: [-0.9916,  0.9898]\n",
      "  dataset_65_s1.csv         -> dataset_65_s1_norm.csv         | Shape: (40094, 12) | Range: [-0.9963,  0.9944]\n",
      "  dataset_66_s1.csv         -> dataset_66_s1_norm.csv         | Shape: (36476, 12) | Range: [-0.9969,  1.0000]\n",
      "  dataset_67_s1.csv         -> dataset_67_s1_norm.csv         | Shape: (11135, 12) | Range: [-0.9889,  0.9904]\n",
      "  dataset_68_s1.csv         -> dataset_68_s1_norm.csv         | Shape: (23331, 12) | Range: [-0.9965,  0.9907]\n",
      "  dataset_69_s1.csv         -> dataset_69_s1_norm.csv         | Shape: (15350, 12) | Range: [-0.9923,  0.9892]\n",
      "  dataset_6_s1.csv          -> dataset_6_s1_norm.csv          | Shape: (40388, 12) | Range: [-1.0000,  0.9995]\n",
      "  dataset_70_s1.csv         -> dataset_70_s1_norm.csv         | Shape: (25677, 12) | Range: [-0.9943,  0.9902]\n",
      "  dataset_71_s1.csv         -> dataset_71_s1_norm.csv         | Shape: (14656, 12) | Range: [-0.9880,  0.9913]\n",
      "  dataset_72_s1.csv         -> dataset_72_s1_norm.csv         | Shape: (15301, 12) | Range: [-0.9792,  0.9923]\n",
      "  dataset_73_s1.csv         -> dataset_73_s1_norm.csv         | Shape: (16786, 12) | Range: [-0.9934,  1.0000]\n",
      "  dataset_74_s1.csv         -> dataset_74_s1_norm.csv         | Shape: (23761, 12) | Range: [-0.9965,  0.9945]\n",
      "  dataset_75_s1.csv         -> dataset_75_s1_norm.csv         | Shape: (13472, 12) | Range: [-0.9941,  0.9903]\n",
      "  dataset_76_s1.csv         -> dataset_76_s1_norm.csv         | Shape: (22188, 12) | Range: [-0.9969,  0.9909]\n",
      "  dataset_78_s1.csv         -> dataset_78_s1_norm.csv         | Shape: (8445, 12) | Range: [-0.9948,  0.9871]\n",
      "  dataset_79_s1.csv         -> dataset_79_s1_norm.csv         | Shape: (31154, 12) | Range: [-0.9946,  0.9931]\n",
      "  dataset_7_s1.csv          -> dataset_7_s1_norm.csv          | Shape: (14651, 12) | Range: [-0.1680,  0.7754]\n",
      "  dataset_80_s1.csv         -> dataset_80_s1_norm.csv         | Shape: (23824, 12) | Range: [-0.9938,  0.9962]\n",
      "  dataset_81_s1.csv         -> dataset_81_s1_norm.csv         | Shape: (17672, 12) | Range: [-0.9881,  0.9966]\n",
      "  dataset_8_s1.csv          -> dataset_8_s1_norm.csv          | Shape: (18757, 12) | Range: [-0.4904,  0.8073]\n",
      "  dataset_9_s1.csv          -> dataset_9_s1_norm.csv          | Shape: (20336, 12) | Range: [-0.7641,  0.8997]\n",
      "\n",
      "Normalization complete!\n",
      "✅ Successfully processed: 69/69 files\n",
      "📁 Normalized datasets saved in EM_data_norm/\n",
      "\n",
      "Summary:\n",
      "- Input folder: EM_data/\n",
      "- Output folder: EM_data_norm/\n",
      "- Files processed: 69/69\n",
      "- Total rows processed: 1,330,816\n",
      "- Columns: ['u_q', 'coolant', 'stator_winding', 'u_d', 'stator_tooth', 'motor_speed', 'i_d', 'i_q', 'pm', 'stator_yoke', 'ambient', 'torque']\n",
      "- Normalization: col_norm = col / max(abs(col_all_datasets))\n",
      "\n",
      "======================================================================\n",
      "VERIFICATION: Checking normalization correctness\n",
      "======================================================================\n",
      "Found 69 normalized files\n",
      "✅ dataset_10_s1_norm.csv         | Range: [-0.9968,  0.9995]\n",
      "✅ dataset_11_s1_norm.csv         | Range: [-0.9888,  0.9897]\n",
      "✅ dataset_12_s1_norm.csv         | Range: [-0.6494,  0.7863]\n",
      "... and 66 more files\n",
      "\n",
      "Detailed statistics for: dataset_10_s1_norm.csv\n",
      "Shape: (15256, 12)\n",
      "\n",
      "Column ranges (should be within [-1, 1]):\n",
      "  u_q            : [-0.0300,  0.8713] ✅\n",
      "  coolant        : [ 0.1722,  0.1974] ✅\n",
      "  stator_winding : [ 0.1342,  0.6627] ✅\n",
      "  u_d            : [-0.9968,  0.9991] ✅\n",
      "  stator_tooth   : [ 0.1632,  0.6164] ✅\n",
      "  motor_speed    : [-0.0051,  0.9995] ✅\n",
      "  i_d            : [-0.8559, -0.0072] ✅\n",
      "  i_q            : [-0.5297,  0.5734] ✅\n",
      "  pm             : [ 0.2018,  0.6345] ✅\n",
      "  stator_yoke    : [ 0.1812,  0.4551] ✅\n",
      "  ambient        : [ 0.6463,  0.7660] ✅\n",
      "  torque         : [-0.4936,  0.5369] ✅\n",
      "\n",
      "🎉 Process completed successfully!\n",
      "All normalized values are within [-1, 1] range.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def normalize_datasets():\n",
    "    \"\"\"\n",
    "    Load all CSV files from EM_data/, find global absolute maxima across all datasets,\n",
    "    normalize each dataset by global absolute maxima, and save to EM_data_norm/\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get all CSV files in EM_data folder\n",
    "    input_folder = 'EM_data'\n",
    "    csv_pattern = os.path.join(input_folder, '*.csv')\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {input_folder}/ folder!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files:\")\n",
    "    for file in csv_files:\n",
    "        print(f\"  - {os.path.basename(file)}\")\n",
    "    \n",
    "    # Step 1: Load all datasets and concatenate to find global absolute maxima\n",
    "    print(\"\\nStep 1: Loading all datasets to find global absolute maxima...\")\n",
    "    all_data = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            all_data.append(df)\n",
    "            total_rows += len(df)\n",
    "            print(f\"  Loaded {os.path.basename(file)}: {len(df)} rows, {len(df.columns)} columns\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading {file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"No datasets loaded successfully!\")\n",
    "        return\n",
    "    \n",
    "    # Concatenate all data to find global absolute maxima\n",
    "    print(f\"\\nCombining {len(all_data)} datasets ({total_rows} total rows)...\")\n",
    "    combined_data = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Find maximum of absolute values for each column\n",
    "    global_abs_max = combined_data.abs().max()\n",
    "    \n",
    "    print(f\"\\nGlobal absolute maxima across all datasets:\")\n",
    "    print(\"=\" * 60)\n",
    "    for col, max_val in global_abs_max.items():\n",
    "        min_val = combined_data[col].min()\n",
    "        max_val_orig = combined_data[col].max()\n",
    "        print(f\"{col:15}: max(abs) = {max_val:12.6f} | range: [{min_val:8.3f}, {max_val_orig:8.3f}]\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 2: Create output folder\n",
    "    output_folder = 'EM_data_norm'\n",
    "    Path(output_folder).mkdir(exist_ok=True)\n",
    "    print(f\"\\nCreated/verified output folder: {output_folder}/\")\n",
    "    \n",
    "    # Step 3: Normalize each dataset and save\n",
    "    print(f\"\\nStep 2: Normalizing and saving datasets...\")\n",
    "    \n",
    "    successful_files = 0\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            # Load dataset\n",
    "            df = pd.read_csv(file)\n",
    "            \n",
    "            # Normalize by global absolute maxima (element-wise division)\n",
    "            df_norm = df / global_abs_max\n",
    "            \n",
    "            # Verify normalization (all values should be between -1 and 1)\n",
    "            assert (df_norm >= -1.001).all().all(), f\"Values < -1 found in {file}\"  # Small tolerance for floating point\n",
    "            assert (df_norm <= 1.001).all().all(), f\"Values > 1 found in {file}\"   # Small tolerance for floating point\n",
    "            \n",
    "            # Create output filename\n",
    "            filename = os.path.basename(file)\n",
    "            name_without_ext = os.path.splitext(filename)[0]\n",
    "            output_filename = f\"{name_without_ext}_norm.csv\"\n",
    "            output_path = os.path.join(output_folder, output_filename)\n",
    "            \n",
    "            # Save normalized dataset\n",
    "            df_norm.to_csv(output_path, index=False)\n",
    "            \n",
    "            # Print statistics\n",
    "            min_val = df_norm.min().min()\n",
    "            max_val = df_norm.max().max()\n",
    "            print(f\"  {filename:25} -> {output_filename:30} | Shape: {df_norm.shape} | Range: [{min_val:7.4f}, {max_val:7.4f}]\")\n",
    "            \n",
    "            successful_files += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error processing {file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nNormalization complete!\")\n",
    "    print(f\"✅ Successfully processed: {successful_files}/{len(csv_files)} files\")\n",
    "    print(f\"📁 Normalized datasets saved in {output_folder}/\")\n",
    "    \n",
    "    # Step 4: Verification summary\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"- Input folder: {input_folder}/\")\n",
    "    print(f\"- Output folder: {output_folder}/\")\n",
    "    print(f\"- Files processed: {successful_files}/{len(csv_files)}\")\n",
    "    print(f\"- Total rows processed: {total_rows:,}\")\n",
    "    print(f\"- Columns: {list(global_abs_max.index)}\")\n",
    "    print(f\"- Normalization: col_norm = col / max(abs(col_all_datasets))\")\n",
    "    \n",
    "    return global_abs_max\n",
    "\n",
    "def verify_normalization():\n",
    "    \"\"\"\n",
    "    Verify that normalization was done correctly by checking a few samples\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VERIFICATION: Checking normalization correctness\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get normalized files\n",
    "    norm_files = glob.glob('EM_data_norm/*_norm.csv')\n",
    "    \n",
    "    if not norm_files:\n",
    "        print(\"No normalized files found!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(norm_files)} normalized files\")\n",
    "    \n",
    "    # Check all files for range compliance\n",
    "    all_in_range = True\n",
    "    for norm_file in norm_files[:3]:  # Check first 3 files as samples\n",
    "        df_norm = pd.read_csv(norm_file)\n",
    "        min_val = df_norm.min().min()\n",
    "        max_val = df_norm.max().max()\n",
    "        \n",
    "        in_range = (-1.001 <= min_val <= 1.001) and (-1.001 <= max_val <= 1.001)\n",
    "        status = \"✅\" if in_range else \"❌\"\n",
    "        \n",
    "        print(f\"{status} {os.path.basename(norm_file):30} | Range: [{min_val:7.4f}, {max_val:7.4f}]\")\n",
    "        \n",
    "        if not in_range:\n",
    "            all_in_range = False\n",
    "    \n",
    "    if len(norm_files) > 3:\n",
    "        print(f\"... and {len(norm_files) - 3} more files\")\n",
    "    \n",
    "    # Detailed check on first file\n",
    "    if norm_files:\n",
    "        sample_file = norm_files[0]\n",
    "        df_norm = pd.read_csv(sample_file)\n",
    "        \n",
    "        print(f\"\\nDetailed statistics for: {os.path.basename(sample_file)}\")\n",
    "        print(f\"Shape: {df_norm.shape}\")\n",
    "        print(f\"\\nColumn ranges (should be within [-1, 1]):\")\n",
    "        for col in df_norm.columns:\n",
    "            min_val = df_norm[col].min()\n",
    "            max_val = df_norm[col].max()\n",
    "            status = \"✅\" if (-1.001 <= min_val <= 1.001) and (-1.001 <= max_val <= 1.001) else \"❌\"\n",
    "            print(f\"  {col:15}: [{min_val:7.4f}, {max_val:7.4f}] {status}\")\n",
    "    \n",
    "    return all_in_range\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Starting dataset normalization process...\")\n",
    "    print(\"Normalization formula: col_norm = col / max(abs(col_all_datasets))\")\n",
    "    print(\"Expected range: [-1, 1]\\n\")\n",
    "    \n",
    "    # Run normalization\n",
    "    global_maxima = normalize_datasets()\n",
    "    \n",
    "    if global_maxima is not None:\n",
    "        # Run verification\n",
    "        verification_passed = verify_normalization()\n",
    "        \n",
    "        if verification_passed:\n",
    "            print(f\"\\n🎉 Process completed successfully!\")\n",
    "            print(f\"All normalized values are within [-1, 1] range.\")\n",
    "        else:\n",
    "            print(f\"\\n⚠️  Process completed with warnings!\")\n",
    "            print(f\"Some values may be outside expected range.\")\n",
    "    else:\n",
    "        print(f\"\\n❌ Process failed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCDF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

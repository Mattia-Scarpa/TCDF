{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3990a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e719f657",
   "metadata": {},
   "source": [
    "#### Depthwise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "979ea9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"PyTorch does not offer native support for causal convolutions, so it is implemented (with some inefficiency) by simply using a standard convolution with zero padding on both sides, and chopping off the end of the sequence.\"\"\"\n",
    "    def __init__(self, chomp_size) -> None:\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "class FirstBlock(nn.Module):\n",
    "    def __init__(self, target, n_inputs, n_outputs, kernel_size, stride, dilation, padding, device = 'cpu'):\n",
    "        super(FirstBlock, self).__init__()\n",
    "        \n",
    "        self.target = target\n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation, groups=n_outputs, device=device)\n",
    "\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1)      \n",
    "        self.relu = nn.PReLU(n_inputs, device=device)\n",
    "        self.device = device\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        self.conv1.weight.data.normal_(0, 0.1, generator=torch.Generator(device=self.device))\n",
    "        \n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        out = self.net(x)\n",
    "        return self.relu(out)    \n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, device='cpu') -> None:\n",
    "        super(TemporalBlock, self).__init__()\n",
    "       \n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation, groups=n_outputs, device=device)\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1)\n",
    "        self.relu = nn.PReLU(n_inputs, device=device)\n",
    "        self.device = device\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        self.conv1.weight.data.normal_(0, 0.1, generator=torch.Generator(device=self.device)) \n",
    "        \n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"Forward residual pass of the temporal block\"\"\"\n",
    "        out = self.net(x)\n",
    "        return self.relu(out+x) #residual connection\n",
    "\n",
    "class LastBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, device ='cpu') -> None:\n",
    "        super(LastBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation, groups=n_outputs, device=device)\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1)\n",
    "        self.linear = nn.Linear(n_inputs, n_inputs, device=device)\n",
    "        self.device = device\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        self.linear.weight.data.normal_(0, 0.01, generator=torch.Generator(device=self.device)) \n",
    "        \n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        out = self.net(x)\n",
    "        return self.linear(out.transpose(1,2)+x.transpose(1,2)).transpose(1,2) #residual connection\n",
    "\n",
    "class DepthwiseNet(nn.Module):\n",
    "    def __init__(self, target, num_inputs, num_levels, kernel_size=2, dilation_c=2, device='cpu') -> None:\n",
    "        super(DepthwiseNet, self).__init__()\n",
    "        layers = []\n",
    "        in_channels = num_inputs\n",
    "        out_channels = num_inputs\n",
    "        for l in range(num_levels):\n",
    "            dilation_size = dilation_c ** l\n",
    "            if l==0:\n",
    "                layers += [FirstBlock(target, in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, device=device)]\n",
    "            elif l==num_levels-1:\n",
    "                layers+=[LastBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, device=device)]\n",
    "            \n",
    "            else:\n",
    "                layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, device=device)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the network\"\"\"\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf198ec",
   "metadata": {},
   "source": [
    "### AD-DSTCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2464c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq\n",
    "import copy\n",
    "\n",
    "class ADDSTCN(nn.Module):\n",
    "    def __init__(self, target, input_size, num_levels, kernel_size, cuda, dilation_c,\n",
    "                 lasso_lambda=1e-6):\n",
    "        super(ADDSTCN, self).__init__()\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "        self.lasso_lambda = lasso_lambda\n",
    "        \n",
    "        self.target=target\n",
    "        self.dwn = DepthwiseNet(self.target, input_size, num_levels, kernel_size=kernel_size, dilation_c=dilation_c)\n",
    "        self.pointwise = nn.Conv1d(input_size, 1, 1, device=self.device)\n",
    "\n",
    "        #self._attention = torch.ones(input_size,1)\n",
    "        self.fs_attention_logits = nn.Parameter(torch.ones(input_size, 1, device=self.device))\n",
    "        \n",
    "        if cuda:\n",
    "            self.dwn = self.dwn.cuda()\n",
    "                  \n",
    "    def init_weights(self) -> None:\n",
    "        self.pointwise.weight.data.normal_(0, 0.1)       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # new variable for LASSO attention scores interpretability\n",
    "        attention_scores = torch.sigmoid(self.fs_attention_logits)\n",
    "        \n",
    "        #y1=self.dwn(x*F.softmax(self.fs_attention, dim=0))\n",
    "        y1=self.dwn(x*attention_scores)\n",
    "        y1 = self.pointwise(y1) \n",
    "        return y1.transpose(1,2)\n",
    "    \n",
    "    def attention_regularization(self, p=1):\n",
    "        \"\"\"L1 penalty on attention scores [0,1]\"\"\"\n",
    "        attention_scores = torch.sigmoid(self.fs_attention_logits)\n",
    "        return self.lasso_lambda * torch.norm(attention_scores, p=1)\n",
    "    \n",
    "    def get_sparsity_stats(self):\n",
    "        scores = torch.sigmoid(self.fs_attention_logits).detach().squeeze()\n",
    "        \n",
    "        return {\n",
    "            'mean_attention': scores.mean().item(),\n",
    "            'near_zero': (scores < 0.1).sum().item(),  # \"Spente\"\n",
    "            'active': (scores > 0.5).sum().item(),     # \"Attive\"\n",
    "            'active_idx': (scores > 0.5).nonzero(as_tuple=True)[0].cpu().numpy(),\n",
    "            'sparsity_ratio': (scores < 0.1).float().mean().item()\n",
    "        }\n",
    "        \n",
    "    def get_attention_scores(self):\n",
    "        \"\"\"Returns attention scores as a numpy array.\"\"\"\n",
    "        scores = torch.sigmoid(self.fs_attention_logits).detach().cpu().numpy()\n",
    "        return scores.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5b003b",
   "metadata": {},
   "source": [
    "### TCDF Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6039bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyexpat import model\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def preparedata(file, target) -> tuple[Variable, Variable]:\n",
    "    \"\"\"Reads data from csv file and transforms it to two PyTorch tensors: dataset x and target time series y that has to be predicted.\"\"\"\n",
    "    df_data = pd.read_csv(file)\n",
    "    df_y = df_data.copy(deep=True)[[target]]\n",
    "    df_x = df_data.copy(deep=True)\n",
    "    df_yshift = df_y.copy(deep=True).shift(periods=1, axis=0)\n",
    "    df_yshift[target]=df_yshift[target].fillna(0.)\n",
    "    df_x[target] = df_yshift\n",
    "    data_x = df_x.values.astype('float32').transpose()    \n",
    "    data_y = df_y.values.astype('float32').transpose()\n",
    "    data_x = torch.from_numpy(data_x)\n",
    "    data_y = torch.from_numpy(data_y)\n",
    "\n",
    "    x, y = Variable(data_x), Variable(data_y)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def train(epoch, traindata, traintarget, modelname: ADDSTCN, optimizer,log_interval,epochs):\n",
    "    \"\"\"Trains model by performing one epoch and returns attention scores and loss.\"\"\"\n",
    "\n",
    "    modelname.train()\n",
    "    x, y = traindata[0:1], traintarget[0:1]\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    epochpercentage = (epoch/float(epochs))*100\n",
    "    output = modelname(x)\n",
    "\n",
    "    \n",
    "    loss_mse = F.mse_loss(output, y)\n",
    "    loss_lasso = modelname.attention_regularization()\n",
    "    loss = loss_mse + loss_lasso\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # update lasso lambda\n",
    "    modelname.lasso_lambda = modelname.lasso_lambda * 0.9999#min(1e-9, modelname.lasso_lambda * 0.9999)\n",
    "    \n",
    "    attentionscores = modelname.get_attention_scores()\n",
    "\n",
    "    if epoch % log_interval ==0 or epoch % epochs == 0 or epoch==1:\n",
    "        print('Epoch: {:2d} [{:.0f}%] \\tLoss: {:.4e} - \\tLambda: {:.4e}'.format(epoch, epochpercentage, loss, modelname.lasso_lambda))\n",
    "        print(\"Get sparsity stats: \", modelname.get_sparsity_stats())\n",
    "\n",
    "    return attentionscores, loss\n",
    "\n",
    "def findcauses(target, cuda, epochs, kernel_size, layers, \n",
    "               log_interval, lr, optimizername, seed, dilation_c, significance, file):\n",
    "    \"\"\"Discovers potential causes of one target time series, validates these potential causes with PIVM and discovers the corresponding time delays\"\"\"\n",
    "\n",
    "    print(\"\\n\", \"Analysis started for target: \", target)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    X_train, Y_train = preparedata(file, target)\n",
    "    X_train = X_train.unsqueeze(0).contiguous()\n",
    "    Y_train = Y_train.unsqueeze(2).contiguous()\n",
    "\n",
    "    input_channels = X_train.size()[1]\n",
    "       \n",
    "    targetidx = pd.read_csv(file).columns.get_loc(target)\n",
    "          \n",
    "    model = ADDSTCN(targetidx, input_channels, layers, kernel_size=kernel_size, cuda=cuda, dilation_c=dilation_c)\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "        X_train = X_train.cuda()\n",
    "        Y_train = Y_train.cuda()\n",
    "\n",
    "    optimizer = getattr(optim, optimizername)(model.parameters(), lr=lr)    \n",
    "    \n",
    "    scores, firstloss = train(1, X_train, Y_train, model, optimizer,log_interval,epochs)\n",
    "    firstloss = firstloss.cpu().data.item()\n",
    "    for ep in range(2, epochs+1):\n",
    "        scores, realloss = train(ep, X_train, Y_train, model, optimizer,log_interval,epochs)\n",
    "        #if ep == 10:\n",
    "        #    firstloss = realloss.cpu().data.item()\n",
    "    realloss = realloss.cpu().data.item()\n",
    "    \n",
    "    print(\"Scores: \", scores)\n",
    "    print(\"First loss: \", firstloss, \" - Real loss: \", realloss)\n",
    "    print('-'*50)\n",
    "    \n",
    "    #s = sorted(scores.view(-1).cpu().detach().numpy(), reverse=True)\n",
    "    #print(\"Scores: \", s)\n",
    "    #indices = np.argsort(-1 * scores.view(-1).cpu().detach().numpy()) # -1 for descending order\n",
    "    #print(\"Indices: \", indices)\n",
    "    \n",
    "    # get indices of scores that are greater than 0.5\n",
    "    potentials = list(np.where(scores > 0.5)[0])\n",
    "    \n",
    "    #attention interpretation to find tau: the threshold that distinguishes potential causes from non-causal time series\n",
    "    \n",
    "    #if len(s)<=5:\n",
    "    #    potentials = []\n",
    "    #    for i in indices:\n",
    "    #        if scores[i]>1.:\n",
    "    #            potentials.append(i)\n",
    "    #else:\n",
    "    #    potentials = []\n",
    "    #    gaps = []\n",
    "    #    for i in range(len(s)-1):\n",
    "    #        if s[i]<1.: #tau should be greater or equal to 1, so only consider scores >= 1\n",
    "    #            break\n",
    "    #        gap = s[i]-s[i+1]\n",
    "    #        gaps.append(gap)\n",
    "    #    sortgaps = sorted(gaps, reverse=True)\n",
    "    #    \n",
    "    #    for i in range(0, len(gaps)):\n",
    "    #        largestgap = sortgaps[i]\n",
    "    #        index = gaps.index(largestgap)\n",
    "    #        ind = -1\n",
    "    #        if index<((len(s)-1)/2): #gap should be in first half\n",
    "    #            if index>2:\n",
    "    #                ind=index #gap should have index > 0, except if second score <1\n",
    "    #                break\n",
    "    #    if ind<0:\n",
    "    #        ind = 0\n",
    "    #            \n",
    "    #    potentials = indices[:ind+1].tolist()\n",
    "    print(\"Potential causes: \", potentials)\n",
    "    validated = copy.deepcopy(potentials)\n",
    "    \n",
    "    #Apply PIVM (permutes the values) to check if potential cause is true cause\n",
    "    for idx in potentials:\n",
    "        random.seed(seed)\n",
    "        X_test2 = X_train.clone().cpu().numpy()\n",
    "        random.shuffle(X_test2[:,idx,:][0])\n",
    "        shuffled = torch.from_numpy(X_test2)\n",
    "        if cuda:\n",
    "            shuffled=shuffled.cuda()\n",
    "        model.eval()\n",
    "        output = model(shuffled)\n",
    "        testloss = F.mse_loss(output, Y_train)\n",
    "        testloss = testloss.cpu().data.item()\n",
    "        \n",
    "        diff = firstloss-realloss\n",
    "        testdiff = firstloss-testloss\n",
    "        \n",
    "        # debugging\n",
    "        print(\"Potential cause: \", idx, \" - Test loss: \", testloss, \" - Diff: \", testdiff, \" - Significance: \", significance)\n",
    "        print(\"First loss: \", firstloss, \" - Real loss: \", realloss, \" - Diff: \", diff)\n",
    "\n",
    "        if testdiff>(diff*significance): \n",
    "            validated.remove(idx) \n",
    "    \n",
    " \n",
    "    weights = []\n",
    "    \n",
    "    #Discover time delay between cause and effect by interpreting kernel weights\n",
    "    for layer in range(layers):\n",
    "        weight = model.dwn.network[layer].net[0].weight.abs().view(model.dwn.network[layer].net[0].weight.size()[0], model.dwn.network[layer].net[0].weight.size()[2])\n",
    "        weights.append(weight)\n",
    "\n",
    "    causeswithdelay = dict()    \n",
    "    for v in validated: \n",
    "        totaldelay=0    \n",
    "        for k in range(len(weights)):\n",
    "            w=weights[k]\n",
    "            row = w[v]\n",
    "            twolargest = heapq.nlargest(2, row)\n",
    "            m = twolargest[0]\n",
    "            m2 = twolargest[1]\n",
    "            if m > m2:\n",
    "                index_max = len(row) - 1 - max(range(len(row)), key=row.__getitem__)\n",
    "            else:\n",
    "                #take first filter\n",
    "                index_max=0\n",
    "            delay = index_max *(dilation_c**k)\n",
    "            totaldelay+=delay\n",
    "        if targetidx != v:\n",
    "            causeswithdelay[(targetidx, v)]=totaldelay\n",
    "        else:\n",
    "            causeswithdelay[(targetidx, v)]=totaldelay+1\n",
    "    print(\"Validated causes: \", validated)\n",
    "    \n",
    "    return validated, causeswithdelay, realloss, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7ccec",
   "metadata": {},
   "source": [
    "### Run TCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f71503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pylab\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# os.chdir(os.path.dirname(sys.argv[0])) #uncomment this line to run in VSCode\n",
    "\n",
    "def check_positive(value):\n",
    "    \"\"\"Checks if argument is positive integer (larger than zero).\"\"\"\n",
    "    ivalue = int(value)\n",
    "    if ivalue <= 0:\n",
    "         raise argparse.ArgumentTypeError(\"%s should be positive\" % value)\n",
    "    return ivalue\n",
    "\n",
    "def check_zero_or_positive(value):\n",
    "    \"\"\"Checks if argument is positive integer (larger than or equal to zero).\"\"\"\n",
    "    ivalue = int(value)\n",
    "    if ivalue < 0:\n",
    "         raise argparse.ArgumentTypeError(\"%s should be positive\" % value)\n",
    "    return ivalue\n",
    "\n",
    "class StoreDictKeyPair(argparse.Action):\n",
    "    \"\"\"Creates dictionary containing datasets as keys and ground truth files as values.\"\"\"\n",
    "    def __call__(self, parser, namespace, values, option_string=None):\n",
    "        my_dict = {}\n",
    "        for kv in values.split(\",\"):\n",
    "            k,v = kv.split(\"=\")\n",
    "            my_dict[k] = v\n",
    "        setattr(namespace, self.dest, my_dict)\n",
    "\n",
    "def getextendeddelays(gtfile, columns):\n",
    "    \"\"\"Collects the total delay of indirect causal relationships.\"\"\"\n",
    "    gtdata = pd.read_csv(gtfile, header=None)\n",
    "\n",
    "    readgt=dict()\n",
    "    effects = gtdata[1]\n",
    "    causes = gtdata[0]\n",
    "    delays = gtdata[2]\n",
    "    gtnrrelations = 0\n",
    "    pairdelays = dict()\n",
    "    for k in range(len(columns)):\n",
    "        readgt[k]=[]\n",
    "    for i in range(len(effects)):\n",
    "        key=effects[i]\n",
    "        value=causes[i]\n",
    "        readgt[key].append(value)\n",
    "        pairdelays[(key, value)]=delays[i]\n",
    "        gtnrrelations+=1\n",
    "    \n",
    "    g = nx.DiGraph()\n",
    "    g.add_nodes_from(readgt.keys())\n",
    "    for e in readgt:\n",
    "        cs = readgt[e]\n",
    "        for c in cs:\n",
    "            g.add_edge(c, e)\n",
    "\n",
    "    extendedreadgt = copy.deepcopy(readgt)\n",
    "    \n",
    "    for c1 in range(len(columns)):\n",
    "        for c2 in range(len(columns)):\n",
    "            paths = list(nx.all_simple_paths(g, c1, c2, cutoff=2)) #indirect path max length 3, no cycles\n",
    "            \n",
    "            if len(paths)>0:\n",
    "                for path in paths:\n",
    "                    for p in path[:-1]:\n",
    "                        if p not in extendedreadgt[path[-1]]:\n",
    "                            extendedreadgt[path[-1]].append(p)\n",
    "                            \n",
    "    extendedgtdelays = dict()\n",
    "    for effect in extendedreadgt:\n",
    "        causes = extendedreadgt[effect]\n",
    "        for cause in causes:\n",
    "            if (effect, cause) in pairdelays:\n",
    "                delay = pairdelays[(effect, cause)]\n",
    "                extendedgtdelays[(effect, cause)]=[delay]\n",
    "            else:\n",
    "                #find extended delay\n",
    "                paths = list(nx.all_simple_paths(g, cause, effect, cutoff=2)) #indirect path max length 3, no cycles\n",
    "                extendedgtdelays[(effect, cause)]=[]\n",
    "                for p in paths:\n",
    "                    delay=0\n",
    "                    for i in range(len(p)-1):\n",
    "                        delay+=pairdelays[(p[i+1], p[i])]\n",
    "                    extendedgtdelays[(effect, cause)].append(delay)\n",
    "\n",
    "    return extendedgtdelays, readgt, extendedreadgt\n",
    "\n",
    "def evaluate(gtfile, validatedcauses, columns):\n",
    "    \"\"\"Evaluates the results of TCDF by comparing it to the ground truth graph, and calculating precision, recall and F1-score. F1'-score, precision' and recall' include indirect causal relationships.\"\"\"\n",
    "    extendedgtdelays, readgt, extendedreadgt = getextendeddelays(gtfile, columns)\n",
    "    FP=0\n",
    "    FPdirect=0\n",
    "    TPdirect=0\n",
    "    TP=0\n",
    "    FN=0\n",
    "    FPs = []\n",
    "    FPsdirect = []\n",
    "    TPsdirect = []\n",
    "    TPs = []\n",
    "    FNs = []\n",
    "    for key in readgt:\n",
    "        for v in validatedcauses[key]:\n",
    "            if v not in extendedreadgt[key]:\n",
    "                FP+=1\n",
    "                FPs.append((key,v))\n",
    "            else:\n",
    "                TP+=1\n",
    "                TPs.append((key,v))\n",
    "            if v not in readgt[key]:\n",
    "                FPdirect+=1\n",
    "                FPsdirect.append((key,v))\n",
    "            else:\n",
    "                TPdirect+=1\n",
    "                TPsdirect.append((key,v))\n",
    "        for v in readgt[key]:\n",
    "            if v not in validatedcauses[key]:\n",
    "                FN+=1\n",
    "                FNs.append((key, v))\n",
    "          \n",
    "    print(\"Total False Positives': \", FP)\n",
    "    print(\"Total True Positives': \", TP)\n",
    "    print(\"Total False Negatives: \", FN)\n",
    "    print(\"Total Direct False Positives: \", FPdirect)\n",
    "    print(\"Total Direct True Positives: \", TPdirect)\n",
    "    print(\"TPs': \", TPs)\n",
    "    print(\"FPs': \", FPs)\n",
    "    print(\"TPs direct: \", TPsdirect)\n",
    "    print(\"FPs direct: \", FPsdirect)\n",
    "    print(\"FNs: \", FNs)\n",
    "    precision = recall = 0.\n",
    "\n",
    "    if float(TP+FP)>0:\n",
    "        precision = TP / float(TP+FP)\n",
    "    print(\"Precision': \", precision)\n",
    "    if float(TP + FN)>0:\n",
    "        recall = TP / float(TP + FN)\n",
    "    print(\"Recall': \", recall)\n",
    "    if (precision + recall) > 0:\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1 = 0.\n",
    "    print(\"F1' score: \", F1,\"(includes direct and indirect causal relationships)\")\n",
    "\n",
    "    precision = recall = 0.\n",
    "    if float(TPdirect+FPdirect)>0:\n",
    "        precision = TPdirect / float(TPdirect+FPdirect)\n",
    "    print(\"Precision: \", precision)\n",
    "    if float(TPdirect + FN)>0:\n",
    "        recall = TPdirect / float(TPdirect + FN)\n",
    "    print(\"Recall: \", recall)\n",
    "    if (precision + recall) > 0:\n",
    "        F1direct = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1direct = 0.\n",
    "    print(\"F1 score: \", F1direct,\"(includes only direct causal relationships)\")\n",
    "    return FP, TP, FPdirect, TPdirect, FN, FPs, FPsdirect, TPs, TPsdirect, FNs, F1, F1direct\n",
    "\n",
    "def evaluatedelay(extendedgtdelays, alldelays, TPs, receptivefield):\n",
    "    \"\"\"Evaluates the delay discovery of TCDF by comparing the discovered time delays with the ground truth.\"\"\"\n",
    "    zeros = 0\n",
    "    total = 0.\n",
    "    for i in range(len(TPs)):\n",
    "        tp=TPs[i]\n",
    "        discovereddelay = alldelays[tp]\n",
    "        gtdelays = extendedgtdelays[tp]\n",
    "        for d in gtdelays:\n",
    "            if d <= receptivefield:\n",
    "                total+=1.\n",
    "                error = d - discovereddelay\n",
    "                if error == 0:\n",
    "                    zeros+=1\n",
    "                \n",
    "            else:\n",
    "                next\n",
    "           \n",
    "    if zeros==0:\n",
    "        return 0.\n",
    "    else:\n",
    "        return zeros/float(total)\n",
    "\n",
    "\n",
    "def runTCDF(datafile):\n",
    "    \"\"\"Loops through all variables in a dataset and return the discovered causes, time delays, losses, attention scores and variable names.\"\"\"\n",
    "    df_data = pd.read_csv(datafile)\n",
    "\n",
    "    allcauses = dict()\n",
    "    alldelays = dict()\n",
    "    allreallosses=dict()\n",
    "    allscores=dict()\n",
    "\n",
    "    columns = list(df_data)\n",
    "    for c in columns:\n",
    "        idx = df_data.columns.get_loc(c)\n",
    "        causes, causeswithdelay, realloss, scores = findcauses(c, cuda=cuda, epochs=nrepochs, \n",
    "        kernel_size=kernel_size, layers=levels, log_interval=loginterval, \n",
    "        lr=learningrate, optimizername=optimizername,\n",
    "        seed=seed, dilation_c=dilation_c, significance=significance, file=datafile)\n",
    "\n",
    "        allscores[idx]=scores\n",
    "        allcauses[idx]=causes\n",
    "        alldelays.update(causeswithdelay)\n",
    "        allreallosses[idx]=realloss\n",
    "\n",
    "    return allcauses, alldelays, allreallosses, allscores, columns\n",
    "\n",
    "def plotgraph(stringdatafile,alldelays,columns):\n",
    "    \"\"\"Plots a temporal causal graph showing all discovered causal relationships annotated with the time delay between cause and effect.\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    for c in columns:\n",
    "        G.add_node(c)\n",
    "    for pair in alldelays:\n",
    "        p1,p2 = pair\n",
    "        nodepair = (columns[p2], columns[p1])\n",
    "\n",
    "        G.add_edges_from([nodepair],weight=alldelays[pair])\n",
    "    \n",
    "    edge_labels=dict([((u,v,),d['weight'])\n",
    "                    for u,v,d in G.edges(data=True)])\n",
    "    \n",
    "    pos=nx.circular_layout(G)\n",
    "    nx.draw_networkx_edge_labels(G,pos,edge_labels=edge_labels)\n",
    "    nx.draw(G,pos, node_color = 'white', edge_color='black',node_size=1000,with_labels = True)\n",
    "    ax = plt.gca()\n",
    "    ax.collections[0].set_edgecolor(\"#200808\") \n",
    "\n",
    "    pylab.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb2c86a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(datafiles, evaluation, levels, kernel_size, dilation_c, plot):\n",
    "    if evaluation:\n",
    "        totalF1direct = [] #contains F1-scores of all datasets\n",
    "        totalF1 = [] #contains F1'-scores of all datasets\n",
    "\n",
    "        receptivefield=1\n",
    "        for l in range(0, levels):\n",
    "            receptivefield+=(kernel_size-1) * dilation_c**(l)\n",
    "\n",
    "    for datafile in datafiles.keys(): \n",
    "        stringdatafile = str(datafile)\n",
    "        if '/' in stringdatafile:\n",
    "            stringdatafile = str(datafile).rsplit('/', 1)[1]\n",
    "        \n",
    "        print(\"\\n Dataset: \", stringdatafile)\n",
    "\n",
    "        # run TCDF\n",
    "        allcauses, alldelays, allreallosses, allscores, columns = runTCDF(datafile) #results of TCDF containing indices of causes and effects\n",
    "\n",
    "        print(\"\\n===================Results for\", stringdatafile,\"==================================\")\n",
    "        for pair in alldelays:\n",
    "            print(columns[pair[1]], \"causes\", columns[pair[0]],\"with a delay of\",alldelays[pair],\"time steps.\")\n",
    "\n",
    "        \n",
    "\n",
    "        if evaluation:\n",
    "            # evaluate TCDF by comparing discovered causes with ground truth\n",
    "            print(\"\\n===================Evaluation for\", stringdatafile,\"===============================\")\n",
    "            FP, TP, FPdirect, TPdirect, FN, FPs, FPsdirect, TPs, TPsdirect, FNs, F1, F1direct = evaluate(datafiles[datafile], allcauses, columns)\n",
    "            totalF1.append(F1)\n",
    "            totalF1direct.append(F1direct)\n",
    "\n",
    "            # evaluate delay discovery\n",
    "            extendeddelays, readgt, extendedreadgt = getextendeddelays(datafiles[datafile], columns)\n",
    "            percentagecorrect = evaluatedelay(extendeddelays, alldelays, TPs, receptivefield)*100\n",
    "            print(\"Percentage of delays that are correctly discovered: \", percentagecorrect,\"%\")\n",
    "            \n",
    "        print(\"==================================================================================\")\n",
    "        \n",
    "        if plot:\n",
    "            plotgraph(stringdatafile, alldelays, columns)\n",
    "\n",
    "    # In case of multiple datasets, calculate average F1-score over all datasets and standard deviation\n",
    "    if len(datafiles.keys())>1 and evaluation:  \n",
    "        print(\"\\nOverall Evaluation: \\n\")      \n",
    "        print(\"F1' scores: \")\n",
    "        for f in totalF1:\n",
    "            print(f)\n",
    "        print(\"Average F1': \", np.mean(totalF1))\n",
    "        print(\"Standard Deviation F1': \", np.std(totalF1),\"\\n\")\n",
    "        print(\"F1 scores: \")\n",
    "        for f in totalF1direct:\n",
    "            print(f)\n",
    "        print(\"Average F1: \", np.mean(totalF1direct))\n",
    "        print(\"Standard Deviation F1: \", np.std(totalF1direct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db42f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  test_norm/motor_simulation_Trj_4_e3_norm.csv\n",
      "\n",
      " Dataset:  motor_simulation_Trj_4_e3_norm.csv\n",
      "\n",
      " Analysis started for target:  Id_current\n",
      "Epoch:  1 [0%] \tLoss: 1.2569e-01 - \tLambda: 9.9990e-07\n",
      "Get sparsity stats:  {'mean_attention': 0.731019139289856, 'near_zero': 0, 'active': 10, 'active_idx': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'sparsity_ratio': 0.0}\n",
      "Epoch: 500 [1%] \tLoss: 8.8001e-04 - \tLambda: 9.5123e-07\n",
      "Get sparsity stats:  {'mean_attention': 0.7361042499542236, 'near_zero': 0, 'active': 10, 'active_idx': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'sparsity_ratio': 0.0}\n"
     ]
    }
   ],
   "source": [
    "kernel_size = 250\n",
    "levels = 1\n",
    "nrepochs = 50000 \n",
    "learningrate = 0.001\n",
    "optimizername = 'Adam'\n",
    "dilation_c = kernel_size\n",
    "loginterval = 500\n",
    "seed= 1111\n",
    "cuda= True\n",
    "significance= .8\n",
    "plot = True\n",
    "ground_truth = None\n",
    "data = ['test_norm/motor_simulation_Trj_4_e3_norm.csv']\n",
    "\n",
    "args_dict = {\n",
    "    'kernel_size': kernel_size,\n",
    "    'levels': levels,\n",
    "    'nrepochs': nrepochs,\n",
    "    'learningrate': learningrate,\n",
    "    'optimizername': optimizername,\n",
    "    'dilation_c': dilation_c,\n",
    "    'loginterval': loginterval,\n",
    "    'seed': seed,\n",
    "    'cuda': cuda,\n",
    "    'significance': significance,\n",
    "    'plot': plot\n",
    "}\n",
    "\n",
    "if ground_truth is not None:\n",
    "    datafiles = ground_truth\n",
    "    main(datafiles, evaluation=True)\n",
    "\n",
    "else:\n",
    "    datafiles = dict()\n",
    "    for dataset in data:\n",
    "        print(\"Dataset: \", dataset)\n",
    "        datafiles[dataset]=\"\"\n",
    "    main(datafiles, evaluation=False, \n",
    "         levels=levels, kernel_size=kernel_size, dilation_c=dilation_c, plot=plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1f16ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

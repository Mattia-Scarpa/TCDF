{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3990a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e719f657",
   "metadata": {},
   "source": [
    "#### Depthwise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "979ea9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"PyTorch does not offer native support for causal convolutions, so it is implemented (with some inefficiency) by simply using a standard convolution with zero padding on both sides, and chopping off the end of the sequence.\"\"\"\n",
    "    def __init__(self, chomp_size) -> None:\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "class FirstBlock(nn.Module):\n",
    "    def __init__(self, target, n_inputs, n_outputs, kernel_size, stride, dilation, padding, device = 'cpu'):\n",
    "        super(FirstBlock, self).__init__()\n",
    "        \n",
    "        self.target = target\n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation, groups=n_outputs, device=device)\n",
    "\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1)      \n",
    "        self.relu = nn.PReLU(n_inputs, device=device)\n",
    "        self.device = device\n",
    "        #self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        self.conv1.weight.data.normal_(0, 0.1, generator=torch.Generator(device=self.device))\n",
    "        \n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        out = self.net(x)\n",
    "        return self.relu(out)    \n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, device='cpu') -> None:\n",
    "        super(TemporalBlock, self).__init__()\n",
    "       \n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation, groups=n_outputs, device=device)\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1)\n",
    "        self.relu = nn.PReLU(n_inputs, device=device)\n",
    "        self.device = device\n",
    "        #self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        self.conv1.weight.data.normal_(0, 0.1, generator=torch.Generator(device=self.device)) \n",
    "        \n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"Forward residual pass of the temporal block\"\"\"\n",
    "        out = self.net(x)\n",
    "        return self.relu(out+x) #residual connection\n",
    "\n",
    "class LastBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, device ='cpu') -> None:\n",
    "        super(LastBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation, groups=n_outputs, device=device)\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1)\n",
    "        self.linear = nn.Linear(n_inputs, n_inputs, device=device)\n",
    "        self.device = device\n",
    "        #self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        self.linear.weight.data.normal_(0, 0.01, generator=torch.Generator(device=self.device)) \n",
    "        \n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        out = self.net(x)\n",
    "        return self.linear(out.transpose(1,2)+x.transpose(1,2)).transpose(1,2) #residual connection\n",
    "\n",
    "class DepthwiseNet(nn.Module):\n",
    "    def __init__(self, target, num_inputs, num_levels, kernel_size=2, dilation_c=2, device='cpu') -> None:\n",
    "        super(DepthwiseNet, self).__init__()\n",
    "        layers = []\n",
    "        in_channels = num_inputs\n",
    "        out_channels = num_inputs\n",
    "        for l in range(num_levels):\n",
    "            dilation_size = dilation_c ** l\n",
    "            if l==0:\n",
    "                layers += [FirstBlock(target, in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, device=device)]\n",
    "            elif l==num_levels-1:\n",
    "                layers+=[LastBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, device=device)]\n",
    "            \n",
    "            else:\n",
    "                layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, device=device)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the network\"\"\"\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf198ec",
   "metadata": {},
   "source": [
    "### AD-DSTCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2464c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq\n",
    "import copy\n",
    "\n",
    "class ADDSTCN(nn.Module):\n",
    "    def __init__(self, target, input_size, num_levels, kernel_size, cuda, dilation_c,\n",
    "                 #lambda_reg=lambda_reg,\n",
    "                ) -> None:\n",
    "        super(ADDSTCN, self).__init__()\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "        self.lasso_lambda = lambda_reg#lasso_lambda\n",
    "        \n",
    "        self.target=target\n",
    "        self.dwn = DepthwiseNet(self.target, input_size, num_levels, kernel_size=kernel_size, dilation_c=dilation_c)\n",
    "        self.pointwise = nn.Conv1d(input_size, 1, 1, device=self.device)\n",
    "\n",
    "        #self._attention = torch.ones(input_size,1)\n",
    "        self.fs_attention_logits = nn.Parameter(torch.ones(input_size, 1, device=self.device))\n",
    "        \n",
    "        if cuda:\n",
    "            self.dwn = self.dwn.cuda()\n",
    "                  \n",
    "    def init_weights(self) -> None:\n",
    "        self.pointwise.weight.data.normal_(0, 0.1)       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # new variable for LASSO attention scores interpretability\n",
    "        attention_scores = torch.sigmoid(self.fs_attention_logits)\n",
    "        \n",
    "        #y1=self.dwn(x*F.softmax(self.fs_attention, dim=0))\n",
    "        y1=self.dwn(x*attention_scores)\n",
    "        y1 = self.pointwise(y1) \n",
    "        return y1.transpose(1,2)\n",
    "    \n",
    "    def attention_regularization(self, p=1):\n",
    "        \"\"\"L1 penalty on attention scores [0,1]\"\"\"\n",
    "        attention_scores = torch.sigmoid(self.fs_attention_logits)\n",
    "        return self.lasso_lambda * torch.norm(attention_scores, p=1)\n",
    "    \n",
    "    def get_sparsity_stats(self):\n",
    "        scores = torch.sigmoid(self.fs_attention_logits).detach().squeeze()\n",
    "        \n",
    "        return {\n",
    "            'mean_attention': scores.mean().item(),\n",
    "            'near_zero': (scores < 0.1).sum().item(),  # \"Spente\"\n",
    "            'active': (scores > 0.5).sum().item(),     # \"Attive\"\n",
    "            'active_idx': (scores > 0.5).nonzero(as_tuple=True)[0].cpu().numpy(),\n",
    "            'sparsity_ratio': (scores < 0.1).float().mean().item()\n",
    "        }\n",
    "        \n",
    "    def get_attention_scores(self):\n",
    "        \"\"\"Returns attention scores as a numpy array.\"\"\"\n",
    "        scores = torch.sigmoid(self.fs_attention_logits).detach().cpu().numpy()\n",
    "        return scores.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1453959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "### NEW ADDSTCN Implementation ###\n",
    "\n",
    "class ADDSTCN_v2(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 target, input_size, num_levels, kernel_size, dilation_c,\n",
    "                 lambda_reg=1e-5, device='cpu'\n",
    "                 ) -> None:\n",
    "        super(ADDSTCN_v2, self).__init__()\n",
    "        \n",
    "        self.device = torch.device(device)\n",
    "        \n",
    "        self.lambda_reg = lambda_reg\n",
    "        \n",
    "        self.target = target\n",
    "        self.dwn = DepthwiseNet(self.target, input_size, num_levels, kernel_size=kernel_size, dilation_c=dilation_c, device=self.device)\n",
    "        self.pointwise = nn.Conv1d(input_size, 1, 1, device=self.device)\n",
    "        \n",
    "        self.fs_attention_logits = nn.Parameter(torch.ones(input_size, 1, device=self.device))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # attention scores transformed to [0,1] for interpretability\n",
    "        attention_scores = torch.sigmoid(self.fs_attention_logits)\n",
    "        \n",
    "        # apply depthwise network with attention scores\n",
    "        y1 = self.dwn(x * attention_scores)\n",
    "        y1 = self.pointwise(y1)\n",
    "        \n",
    "        return y1.transpose(1, 2)\n",
    "    \n",
    "    def attention_regularization(self, p=1):\n",
    "        \"\"\"L1 penalty on attention scores [0,1]\"\"\"\n",
    "        attention_scores = torch.sigmoid(self.fs_attention_logits)\n",
    "        return self.lambda_reg * torch.norm(attention_scores, p=1)\n",
    "    \n",
    "    def get_sparsity_stats(self) -> dict[str, Any]:\n",
    "        scores = torch.sigmoid(self.fs_attention_logits).detach().squeeze()\n",
    "        \n",
    "        return {\n",
    "            'mean_attention': scores.mean().item(),\n",
    "            'near_zero': (scores < 0.1).sum().item(),  # \"Spente\"\n",
    "            'active': (scores > 0.5).sum().item(),     # \"Attive\"\n",
    "            'active_idx': (scores > 0.5).nonzero(as_tuple=True)[0].cpu().numpy(),\n",
    "            'sparsity_ratio': (scores < 0.1).float().mean().item()\n",
    "        }\n",
    "        \n",
    "    def get_attention_scores(self):\n",
    "        \"\"\"Returns attention scores as a numpy array.\"\"\"\n",
    "        scores = torch.sigmoid(self.fs_attention_logits).detach().cpu().numpy()\n",
    "        return scores.squeeze()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5b003b",
   "metadata": {},
   "source": [
    "### TCDF Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6039bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyexpat import model\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def preparedata(file, target) -> tuple[Variable, Variable]:\n",
    "    \"\"\"Reads data from csv file and transforms it to two PyTorch tensors: dataset x and target time series y that has to be predicted.\"\"\"\n",
    "    \n",
    "    df_data_list = \n",
    "    \n",
    "    df_y = df_data.copy(deep=True)[[target]]\n",
    "    df_x = df_data.copy(deep=True)\n",
    "    df_yshift = df_y.copy(deep=True).shift(periods=1, axis=0)\n",
    "    df_yshift[target]=df_yshift[target].fillna(0.)\n",
    "    df_x[target] = df_yshift\n",
    "    data_x = df_x.values.astype('float32').transpose()    \n",
    "    data_y = df_y.values.astype('float32').transpose()\n",
    "    data_x = torch.from_numpy(data_x)\n",
    "    data_y = torch.from_numpy(data_y)\n",
    "\n",
    "    x, y = Variable(data_x), Variable(data_y)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def train(epoch, traindata, traintarget, modelname: ADDSTCN, optimizer,log_interval,epochs):\n",
    "    \"\"\"Trains model by performing one epoch and returns attention scores and loss.\"\"\"\n",
    "\n",
    "    modelname.train()\n",
    "    x, y = traindata[0:1], traintarget[0:1]\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    epochpercentage = (epoch/float(epochs))*100\n",
    "    output = modelname(x)\n",
    "\n",
    "    \n",
    "    loss_mse = F.mse_loss(output, y)\n",
    "    loss_lasso = modelname.attention_regularization()\n",
    "    loss = loss_mse + loss_lasso\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # update lasso lambda\n",
    "    modelname.lasso_lambda = modelname.lasso_lambda * 0.9999#min(1e-9, modelname.lasso_lambda * 0.9999)\n",
    "    \n",
    "    attentionscores = modelname.get_attention_scores()\n",
    "\n",
    "    if epoch % log_interval ==0 or epoch % epochs == 0 or epoch==1:\n",
    "        print('Epoch: {:2d} [{:.0f}%] \\tLoss: {:.4e} - \\tLambda: {:.4e}'.format(epoch, epochpercentage, loss, modelname.lasso_lambda))\n",
    "        print(\"Get sparsity stats: \", modelname.get_sparsity_stats())\n",
    "\n",
    "    return attentionscores, loss\n",
    "\n",
    "def findcauses(target, cuda, epochs, kernel_size, layers, \n",
    "               log_interval, lr, optimizername, seed, dilation_c, significance, file):\n",
    "    \"\"\"Discovers potential causes of one target time series, validates these potential causes with PIVM and discovers the corresponding time delays\"\"\"\n",
    "\n",
    "    print(\"\\n\", \"Analysis started for target: \", target)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    X_train, Y_train = preparedata(file, target)\n",
    "    X_train = X_train.unsqueeze(0).contiguous()\n",
    "    Y_train = Y_train.unsqueeze(2).contiguous()\n",
    "\n",
    "    input_channels = X_train.size()[1]\n",
    "       \n",
    "    targetidx = pd.read_csv(file).columns.get_loc(target)\n",
    "          \n",
    "    model = ADDSTCN(targetidx, input_channels, layers, kernel_size=kernel_size, cuda=cuda, dilation_c=dilation_c)\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "        X_train = X_train.cuda()\n",
    "        Y_train = Y_train.cuda()\n",
    "\n",
    "    optimizer = getattr(optim, optimizername)(model.parameters(), lr=lr)    \n",
    "    \n",
    "    scores, firstloss = train(1, X_train, Y_train, model, optimizer,log_interval,epochs)\n",
    "    firstloss = firstloss.cpu().data.item()\n",
    "    for ep in range(2, epochs+1):\n",
    "        scores, realloss = train(ep, X_train, Y_train, model, optimizer,log_interval,epochs)\n",
    "        #if ep == 10:\n",
    "        #    firstloss = realloss.cpu().data.item()\n",
    "    realloss = realloss.cpu().data.item()\n",
    "    \n",
    "    print(\"Scores: \", scores)\n",
    "    print(\"First loss: \", firstloss, \" - Real loss: \", realloss)\n",
    "    print('-'*50)\n",
    "    \n",
    "    #s = sorted(scores.view(-1).cpu().detach().numpy(), reverse=True)\n",
    "    #print(\"Scores: \", s)\n",
    "    #indices = np.argsort(-1 * scores.view(-1).cpu().detach().numpy()) # -1 for descending order\n",
    "    #print(\"Indices: \", indices)\n",
    "    \n",
    "    # get indices of scores that are greater than 0.5\n",
    "    potentials = list(np.where(scores > 0.5)[0])\n",
    "    \n",
    "    #attention interpretation to find tau: the threshold that distinguishes potential causes from non-causal time series\n",
    "    \n",
    "    #if len(s)<=5:\n",
    "    #    potentials = []\n",
    "    #    for i in indices:\n",
    "    #        if scores[i]>1.:\n",
    "    #            potentials.append(i)\n",
    "    #else:\n",
    "    #    potentials = []\n",
    "    #    gaps = []\n",
    "    #    for i in range(len(s)-1):\n",
    "    #        if s[i]<1.: #tau should be greater or equal to 1, so only consider scores >= 1\n",
    "    #            break\n",
    "    #        gap = s[i]-s[i+1]\n",
    "    #        gaps.append(gap)\n",
    "    #    sortgaps = sorted(gaps, reverse=True)\n",
    "    #    \n",
    "    #    for i in range(0, len(gaps)):\n",
    "    #        largestgap = sortgaps[i]\n",
    "    #        index = gaps.index(largestgap)\n",
    "    #        ind = -1\n",
    "    #        if index<((len(s)-1)/2): #gap should be in first half\n",
    "    #            if index>2:\n",
    "    #                ind=index #gap should have index > 0, except if second score <1\n",
    "    #                break\n",
    "    #    if ind<0:\n",
    "    #        ind = 0\n",
    "    #            \n",
    "    #    potentials = indices[:ind+1].tolist()\n",
    "    print(\"Potential causes: \", potentials)\n",
    "    validated = copy.deepcopy(potentials)\n",
    "    \n",
    "    #Apply PIVM (permutes the values) to check if potential cause is true cause\n",
    "    for idx in potentials:\n",
    "        random.seed(seed)\n",
    "        X_test2 = X_train.clone().cpu().numpy()\n",
    "        random.shuffle(X_test2[:,idx,:][0])\n",
    "        shuffled = torch.from_numpy(X_test2)\n",
    "        if cuda:\n",
    "            shuffled=shuffled.cuda()\n",
    "        model.eval()\n",
    "        output = model(shuffled)\n",
    "        testloss = F.mse_loss(output, Y_train)\n",
    "        testloss = testloss.cpu().data.item()\n",
    "        \n",
    "        diff = firstloss-realloss\n",
    "        testdiff = firstloss-testloss\n",
    "        \n",
    "        # debugging\n",
    "        print(\"Potential cause: \", idx, \" - Test loss: \", testloss, \" - Diff: \", testdiff, \" - Significance: \", significance)\n",
    "        print(\"First loss: \", firstloss, \" - Real loss: \", realloss, \" - Diff: \", diff)\n",
    "\n",
    "        if testdiff>(diff*significance): \n",
    "            validated.remove(idx) \n",
    "    \n",
    " \n",
    "    weights = []\n",
    "    \n",
    "    #Discover time delay between cause and effect by interpreting kernel weights\n",
    "    for layer in range(layers):\n",
    "        weight = model.dwn.network[layer].net[0].weight.abs().view(model.dwn.network[layer].net[0].weight.size()[0], model.dwn.network[layer].net[0].weight.size()[2])\n",
    "        weights.append(weight)\n",
    "\n",
    "    causeswithdelay = dict()    \n",
    "    for v in validated: \n",
    "        totaldelay=0    \n",
    "        for k in range(len(weights)):\n",
    "            w=weights[k]\n",
    "            row = w[v]\n",
    "            twolargest = heapq.nlargest(2, row)\n",
    "            m = twolargest[0]\n",
    "            m2 = twolargest[1]\n",
    "            if m > m2:\n",
    "                index_max = len(row) - 1 - max(range(len(row)), key=row.__getitem__)\n",
    "            else:\n",
    "                #take first filter\n",
    "                index_max=0\n",
    "            delay = index_max *(dilation_c**k)\n",
    "            totaldelay+=delay\n",
    "        if targetidx != v:\n",
    "            causeswithdelay[(targetidx, v)]=totaldelay\n",
    "        else:\n",
    "            causeswithdelay[(targetidx, v)]=totaldelay+1\n",
    "    print(\"Validated causes: \", validated)\n",
    "    \n",
    "    return validated, causeswithdelay, realloss, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7ccec",
   "metadata": {},
   "source": [
    "### Run TCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f71503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pylab\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# os.chdir(os.path.dirname(sys.argv[0])) #uncomment this line to run in VSCode\n",
    "\n",
    "def check_positive(value):\n",
    "    \"\"\"Checks if argument is positive integer (larger than zero).\"\"\"\n",
    "    ivalue = int(value)\n",
    "    if ivalue <= 0:\n",
    "         raise argparse.ArgumentTypeError(\"%s should be positive\" % value)\n",
    "    return ivalue\n",
    "\n",
    "def check_zero_or_positive(value):\n",
    "    \"\"\"Checks if argument is positive integer (larger than or equal to zero).\"\"\"\n",
    "    ivalue = int(value)\n",
    "    if ivalue < 0:\n",
    "         raise argparse.ArgumentTypeError(\"%s should be positive\" % value)\n",
    "    return ivalue\n",
    "\n",
    "class StoreDictKeyPair(argparse.Action):\n",
    "    \"\"\"Creates dictionary containing datasets as keys and ground truth files as values.\"\"\"\n",
    "    def __call__(self, parser, namespace, values, option_string=None):\n",
    "        my_dict = {}\n",
    "        for kv in values.split(\",\"):\n",
    "            k,v = kv.split(\"=\")\n",
    "            my_dict[k] = v\n",
    "        setattr(namespace, self.dest, my_dict)\n",
    "\n",
    "def getextendeddelays(gtfile, columns):\n",
    "    \"\"\"Collects the total delay of indirect causal relationships.\"\"\"\n",
    "    gtdata = pd.read_csv(gtfile, header=None)\n",
    "\n",
    "    readgt=dict()\n",
    "    effects = gtdata[1]\n",
    "    causes = gtdata[0]\n",
    "    delays = gtdata[2]\n",
    "    gtnrrelations = 0\n",
    "    pairdelays = dict()\n",
    "    for k in range(len(columns)):\n",
    "        readgt[k]=[]\n",
    "    for i in range(len(effects)):\n",
    "        key=effects[i]\n",
    "        value=causes[i]\n",
    "        readgt[key].append(value)\n",
    "        pairdelays[(key, value)]=delays[i]\n",
    "        gtnrrelations+=1\n",
    "    \n",
    "    g = nx.DiGraph()\n",
    "    g.add_nodes_from(readgt.keys())\n",
    "    for e in readgt:\n",
    "        cs = readgt[e]\n",
    "        for c in cs:\n",
    "            g.add_edge(c, e)\n",
    "\n",
    "    extendedreadgt = copy.deepcopy(readgt)\n",
    "    \n",
    "    for c1 in range(len(columns)):\n",
    "        for c2 in range(len(columns)):\n",
    "            paths = list(nx.all_simple_paths(g, c1, c2, cutoff=2)) #indirect path max length 3, no cycles\n",
    "            \n",
    "            if len(paths)>0:\n",
    "                for path in paths:\n",
    "                    for p in path[:-1]:\n",
    "                        if p not in extendedreadgt[path[-1]]:\n",
    "                            extendedreadgt[path[-1]].append(p)\n",
    "                            \n",
    "    extendedgtdelays = dict()\n",
    "    for effect in extendedreadgt:\n",
    "        causes = extendedreadgt[effect]\n",
    "        for cause in causes:\n",
    "            if (effect, cause) in pairdelays:\n",
    "                delay = pairdelays[(effect, cause)]\n",
    "                extendedgtdelays[(effect, cause)]=[delay]\n",
    "            else:\n",
    "                #find extended delay\n",
    "                paths = list(nx.all_simple_paths(g, cause, effect, cutoff=2)) #indirect path max length 3, no cycles\n",
    "                extendedgtdelays[(effect, cause)]=[]\n",
    "                for p in paths:\n",
    "                    delay=0\n",
    "                    for i in range(len(p)-1):\n",
    "                        delay+=pairdelays[(p[i+1], p[i])]\n",
    "                    extendedgtdelays[(effect, cause)].append(delay)\n",
    "\n",
    "    return extendedgtdelays, readgt, extendedreadgt\n",
    "\n",
    "def evaluate(gtfile, validatedcauses, columns):\n",
    "    \"\"\"Evaluates the results of TCDF by comparing it to the ground truth graph, and calculating precision, recall and F1-score. F1'-score, precision' and recall' include indirect causal relationships.\"\"\"\n",
    "    extendedgtdelays, readgt, extendedreadgt = getextendeddelays(gtfile, columns)\n",
    "    FP=0\n",
    "    FPdirect=0\n",
    "    TPdirect=0\n",
    "    TP=0\n",
    "    FN=0\n",
    "    FPs = []\n",
    "    FPsdirect = []\n",
    "    TPsdirect = []\n",
    "    TPs = []\n",
    "    FNs = []\n",
    "    for key in readgt:\n",
    "        for v in validatedcauses[key]:\n",
    "            if v not in extendedreadgt[key]:\n",
    "                FP+=1\n",
    "                FPs.append((key,v))\n",
    "            else:\n",
    "                TP+=1\n",
    "                TPs.append((key,v))\n",
    "            if v not in readgt[key]:\n",
    "                FPdirect+=1\n",
    "                FPsdirect.append((key,v))\n",
    "            else:\n",
    "                TPdirect+=1\n",
    "                TPsdirect.append((key,v))\n",
    "        for v in readgt[key]:\n",
    "            if v not in validatedcauses[key]:\n",
    "                FN+=1\n",
    "                FNs.append((key, v))\n",
    "          \n",
    "    print(\"Total False Positives': \", FP)\n",
    "    print(\"Total True Positives': \", TP)\n",
    "    print(\"Total False Negatives: \", FN)\n",
    "    print(\"Total Direct False Positives: \", FPdirect)\n",
    "    print(\"Total Direct True Positives: \", TPdirect)\n",
    "    print(\"TPs': \", TPs)\n",
    "    print(\"FPs': \", FPs)\n",
    "    print(\"TPs direct: \", TPsdirect)\n",
    "    print(\"FPs direct: \", FPsdirect)\n",
    "    print(\"FNs: \", FNs)\n",
    "    precision = recall = 0.\n",
    "\n",
    "    if float(TP+FP)>0:\n",
    "        precision = TP / float(TP+FP)\n",
    "    print(\"Precision': \", precision)\n",
    "    if float(TP + FN)>0:\n",
    "        recall = TP / float(TP + FN)\n",
    "    print(\"Recall': \", recall)\n",
    "    if (precision + recall) > 0:\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1 = 0.\n",
    "    print(\"F1' score: \", F1,\"(includes direct and indirect causal relationships)\")\n",
    "\n",
    "    precision = recall = 0.\n",
    "    if float(TPdirect+FPdirect)>0:\n",
    "        precision = TPdirect / float(TPdirect+FPdirect)\n",
    "    print(\"Precision: \", precision)\n",
    "    if float(TPdirect + FN)>0:\n",
    "        recall = TPdirect / float(TPdirect + FN)\n",
    "    print(\"Recall: \", recall)\n",
    "    if (precision + recall) > 0:\n",
    "        F1direct = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1direct = 0.\n",
    "    print(\"F1 score: \", F1direct,\"(includes only direct causal relationships)\")\n",
    "    return FP, TP, FPdirect, TPdirect, FN, FPs, FPsdirect, TPs, TPsdirect, FNs, F1, F1direct\n",
    "\n",
    "def evaluatedelay(extendedgtdelays, alldelays, TPs, receptivefield):\n",
    "    \"\"\"Evaluates the delay discovery of TCDF by comparing the discovered time delays with the ground truth.\"\"\"\n",
    "    zeros = 0\n",
    "    total = 0.\n",
    "    for i in range(len(TPs)):\n",
    "        tp=TPs[i]\n",
    "        discovereddelay = alldelays[tp]\n",
    "        gtdelays = extendedgtdelays[tp]\n",
    "        for d in gtdelays:\n",
    "            if d <= receptivefield:\n",
    "                total+=1.\n",
    "                error = d - discovereddelay\n",
    "                if error == 0:\n",
    "                    zeros+=1\n",
    "                \n",
    "            else:\n",
    "                next\n",
    "           \n",
    "    if zeros==0:\n",
    "        return 0.\n",
    "    else:\n",
    "        return zeros/float(total)\n",
    "\n",
    "\n",
    "def runTCDF(datafile):\n",
    "    \"\"\"Loops through all variables in a dataset and return the discovered causes, time delays, losses, attention scores and variable names.\"\"\"\n",
    "    \n",
    "    df_paths = glob.glob(datafile+'*.csv')\n",
    "    if len(df_paths) == 0:\n",
    "        raise FileNotFoundError(f\"No CSV files found in the specified path: {datafile}\")\n",
    "    \n",
    "    df_data_list = [pd.read_csv(path) for path in df_paths]\n",
    "    \n",
    "    # columns consistency check\n",
    "    if len(df_data_list) > 1:\n",
    "        columns_set = set(df_data_list[0].columns)\n",
    "        for df in df_data_list[1:]:\n",
    "            if set(df.columns) != columns_set:\n",
    "                raise ValueError(\"All CSV files must have the same columns.\")\n",
    "    \n",
    "    df_data = df_data_list[0] #pd.read_csv(datafile)\n",
    "\n",
    "    allcauses = dict()\n",
    "    alldelays = dict()\n",
    "    allreallosses=dict()\n",
    "    allscores=dict()\n",
    "\n",
    "    columns = list(df_data)\n",
    "    for c in columns:\n",
    "        idx = df_data.columns.get_loc(c)\n",
    "        causes, causeswithdelay, realloss, scores = findcauses(c, cuda=cuda, epochs=nrepochs, \n",
    "        kernel_size=kernel_size, layers=levels, log_interval=loginterval, \n",
    "        lr=learningrate, optimizername=optimizername,\n",
    "        seed=seed, dilation_c=dilation_c, significance=significance, file=datafile)\n",
    "\n",
    "        allscores[idx]=scores\n",
    "        allcauses[idx]=causes\n",
    "        alldelays.update(causeswithdelay)\n",
    "        allreallosses[idx]=realloss\n",
    "\n",
    "    return allcauses, alldelays, allreallosses, allscores, columns\n",
    "\n",
    "def plotgraph(stringdatafile,alldelays,columns) -> None:\n",
    "    \"\"\"Plots a temporal causal graph showing all discovered causal relationships annotated with the time delay between cause and effect.\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    for c in columns:\n",
    "        G.add_node(c)\n",
    "    for pair in alldelays:\n",
    "        p1,p2 = pair\n",
    "        nodepair = (columns[p2], columns[p1])\n",
    "\n",
    "        G.add_edges_from([nodepair],weight=alldelays[pair])\n",
    "    \n",
    "    edge_labels=dict([((u,v,),d['weight'])\n",
    "                    for u,v,d in G.edges(data=True)])\n",
    "    \n",
    "    pos=nx.circular_layout(G)\n",
    "    nx.draw_networkx_edge_labels(G,pos,edge_labels=edge_labels)\n",
    "    nx.draw(G,pos, node_color = 'white', edge_color='black',node_size=1000,with_labels = True)\n",
    "    ax = plt.gca()\n",
    "    ax.collections[0].set_edgecolor(\"#200808\") \n",
    "\n",
    "    pylab.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb2c86a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(datafiles, evaluation, levels, kernel_size, dilation_c, plot):\n",
    "    if evaluation:\n",
    "        totalF1direct = [] #contains F1-scores of all datasets\n",
    "        totalF1 = [] #contains F1'-scores of all datasets\n",
    "\n",
    "        receptivefield=1\n",
    "        for l in range(0, levels):\n",
    "            receptivefield+=(kernel_size-1) * dilation_c**(l)\n",
    "\n",
    "    for datafile in datafiles.keys(): \n",
    "        \n",
    "        print(\"\\n Dataset: \", datafile)\n",
    "\n",
    "        # run TCDF\n",
    "        allcauses, alldelays, allreallosses, allscores, columns = runTCDF(datafile) #results of TCDF containing indices of causes and effects\n",
    "\n",
    "        print(\"\\n===================Results for\", datafile,\"==================================\")\n",
    "        for pair in alldelays:\n",
    "            print(columns[pair[1]], \"causes\", columns[pair[0]],\"with a delay of\",alldelays[pair],\"time steps.\")\n",
    "\n",
    "        \n",
    "\n",
    "        if evaluation:\n",
    "            # evaluate TCDF by comparing discovered causes with ground truth\n",
    "            print(\"\\n===================Evaluation for\", datafile,\"===============================\")\n",
    "            FP, TP, FPdirect, TPdirect, FN, FPs, FPsdirect, TPs, TPsdirect, FNs, F1, F1direct = evaluate(datafiles[datafile], allcauses, columns)\n",
    "            totalF1.append(F1)\n",
    "            totalF1direct.append(F1direct)\n",
    "\n",
    "            # evaluate delay discovery\n",
    "            extendeddelays, readgt, extendedreadgt = getextendeddelays(datafiles[datafile], columns)\n",
    "            percentagecorrect = evaluatedelay(extendeddelays, alldelays, TPs, receptivefield)*100\n",
    "            print(\"Percentage of delays that are correctly discovered: \", percentagecorrect,\"%\")\n",
    "            \n",
    "        print(\"==================================================================================\")\n",
    "        \n",
    "        if plot:\n",
    "            plotgraph(datafile, alldelays, columns)\n",
    "\n",
    "    # In case of multiple datasets, calculate average F1-score over all datasets and standard deviation\n",
    "    if len(datafiles.keys())>1 and evaluation:  \n",
    "        print(\"\\nOverall Evaluation: \\n\")      \n",
    "        print(\"F1' scores: \")\n",
    "        for f in totalF1:\n",
    "            print(f)\n",
    "        print(\"Average F1': \", np.mean(totalF1))\n",
    "        print(\"Standard Deviation F1': \", np.std(totalF1),\"\\n\")\n",
    "        print(\"F1 scores: \")\n",
    "        for f in totalF1direct:\n",
    "            print(f)\n",
    "        print(\"Average F1: \", np.mean(totalF1direct))\n",
    "        print(\"Standard Deviation F1: \", np.std(totalF1direct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2db42f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  test_norm/\n",
      "\n",
      " Dataset:  test_norm/\n",
      "Id_current\n",
      "\n",
      " Analysis started for target:  Id_current\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: 'test_norm/'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIsADirectoryError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDataset: \u001b[39m\u001b[33m\"\u001b[39m, dataset)\n\u001b[32m     38\u001b[39m     datafiles[dataset]=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatafiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m     \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation_c\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdilation_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(datafiles, evaluation, levels, kernel_size, dilation_c, plot)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Dataset: \u001b[39m\u001b[33m\"\u001b[39m, datafile)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# run TCDF\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m allcauses, alldelays, allreallosses, allscores, columns = \u001b[43mrunTCDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatafile\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#results of TCDF containing indices of causes and effects\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m===================Results for\u001b[39m\u001b[33m\"\u001b[39m, datafile,\u001b[33m\"\u001b[39m\u001b[33m==================================\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m alldelays:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 216\u001b[39m, in \u001b[36mrunTCDF\u001b[39m\u001b[34m(datafile)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28mprint\u001b[39m(c)\n\u001b[32m    215\u001b[39m idx = df_data.columns.get_loc(c)\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m causes, causeswithdelay, realloss, scores = \u001b[43mfindcauses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m\u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloginterval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearningrate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizername\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizername\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation_c\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdilation_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignificance\u001b[49m\u001b[43m=\u001b[49m\u001b[43msignificance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatafile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m allscores[idx]=scores\n\u001b[32m    222\u001b[39m allcauses[idx]=causes\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mfindcauses\u001b[39m\u001b[34m(target, cuda, epochs, kernel_size, layers, log_interval, lr, optimizername, seed, dilation_c, significance, file)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAnalysis started for target: \u001b[39m\u001b[33m\"\u001b[39m, target)\n\u001b[32m     64\u001b[39m torch.manual_seed(seed)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m X_train, Y_train = \u001b[43mpreparedata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m X_train = X_train.unsqueeze(\u001b[32m0\u001b[39m).contiguous()\n\u001b[32m     68\u001b[39m Y_train = Y_train.unsqueeze(\u001b[32m2\u001b[39m).contiguous()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mpreparedata\u001b[39m\u001b[34m(file, target)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreparedata\u001b[39m(file, target) -> \u001b[38;5;28mtuple\u001b[39m[Variable, Variable]:\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Reads data from csv file and transforms it to two PyTorch tensors: dataset x and target time series y that has to be predicted.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     df_data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     df_y = df_data.copy(deep=\u001b[38;5;28;01mTrue\u001b[39;00m)[[target]]\n\u001b[32m     18\u001b[39m     df_x = df_data.copy(deep=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jax/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jax/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jax/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jax/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jax/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mIsADirectoryError\u001b[39m: [Errno 21] Is a directory: 'test_norm/'"
     ]
    }
   ],
   "source": [
    "kernel_size = 250\n",
    "levels = 1\n",
    "nrepochs = 50000 \n",
    "learningrate = 0.01\n",
    "lambda_reg = 1e-6\n",
    "optimizername = 'Adam'\n",
    "dilation_c = kernel_size\n",
    "loginterval = 500\n",
    "seed= 1111\n",
    "cuda= True\n",
    "significance= .8\n",
    "plot = True\n",
    "ground_truth = None\n",
    "data = ['test_norm/']#['test_norm/motor_simulation_Trj_4_e3_norm.csv']\n",
    "\n",
    "args_dict = {\n",
    "    'kernel_size': kernel_size,\n",
    "    'levels': levels,\n",
    "    'nrepochs': nrepochs,\n",
    "    'learningrate': learningrate,\n",
    "    'optimizername': optimizername,\n",
    "    'dilation_c': dilation_c,\n",
    "    'loginterval': loginterval,\n",
    "    'seed': seed,\n",
    "    'cuda': cuda,\n",
    "    'significance': significance,\n",
    "    'plot': plot\n",
    "}\n",
    "\n",
    "if ground_truth is not None:\n",
    "    datafiles = ground_truth\n",
    "    main(datafiles, evaluation=True)\n",
    "\n",
    "else:\n",
    "    datafiles = dict()\n",
    "    for dataset in data:\n",
    "        print(\"Dataset: \", dataset)\n",
    "        datafiles[dataset]=\"\"\n",
    "    main(datafiles, evaluation=False, \n",
    "         levels=levels, kernel_size=kernel_size, dilation_c=dilation_c, plot=plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1f16ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
